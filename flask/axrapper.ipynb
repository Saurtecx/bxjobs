{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b6e08334",
   "metadata": {},
   "source": [
    "# ************************** Scrapper************************ #"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1ed5cbac",
   "metadata": {},
   "source": [
    "### Loading the required Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2798716e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "import time\n",
    "from selenium.webdriver.common.by import By\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium.webdriver.support.ui import Select\n",
    "import requests\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.support.select import Select\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import urllib.request\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "user_agent = {'User-agent': 'Mozilla/5.0'} \n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.action_chains import ActionChains as ac\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import pyautogui # to press down arrow key\n",
    "from html.parser import HTMLParser \n",
    "import datetime\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import pandas as pd\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1a42c2c6",
   "metadata": {},
   "source": [
    "### Enter the Job you are interested in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2b9fc3b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ScrapingToMongoDb():\n",
    "    import datetime\n",
    "    user_input = input(\"Enter the Job you are interested in: \")\n",
    "\n",
    "    # Print the input string\n",
    "    print(\"You entered:\", user_input)\n",
    "\n",
    "    # Using Selenium to search and Scroll through LinkedIn jobs\n",
    "\n",
    "    browser = webdriver.Chrome(executable_path=\"/usr/local/bin/chromedriver.exe\")\n",
    "    browser.maximize_window()\n",
    "    url= \"https://www.linkedin.com/\"\n",
    "\n",
    "    browser.get(url)\n",
    "    time.sleep(2)\n",
    "\n",
    "    ###################################################################################################\n",
    "    # Open LinkedIn website and click on Jobs\n",
    "    ###################################################################################################\n",
    "\n",
    "    url = \"https://www.linkedin.com/\"\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    #print(soup)\n",
    "\n",
    "    jobs_link = WebDriverWait(browser, 10).until(\n",
    "        EC.element_to_be_clickable((By.XPATH, '/html/body/nav/ul/li[4]/a'))\n",
    "    )\n",
    "    jobs_link.click()\n",
    "\n",
    "    search_bar = WebDriverWait(browser, 10).until(\n",
    "        EC.element_to_be_clickable((By.XPATH, \"//input[@placeholder='Search job titles or companies']\"))\n",
    "    )\n",
    "    search_bar.click()\n",
    "\n",
    "    search_bar.send_keys(user_input) \n",
    "    search_bar2 = WebDriverWait(browser, 10).until(\n",
    "        EC.element_to_be_clickable((By.XPATH, \"//input[@placeholder='Location']\"))\n",
    "    )\n",
    "    search_bar2.click()\n",
    "    search_bar2.clear()\n",
    "    search_bar2.send_keys('India') \n",
    "    search_bar2.send_keys(Keys.RETURN)\n",
    "\n",
    "    time.sleep(5)\n",
    "\n",
    "    ###################################################################################################\n",
    "    # Keep scrolling and load more jobs(5 times)\n",
    "    ###################################################################################################\n",
    "    for i in range(6): # You can change this to scrape for more jobs\n",
    "        pyautogui.press('down',presses=100)\n",
    "        pyautogui.press('down',presses=100)\n",
    "        pyautogui.press('down',presses=100)\n",
    "        pyautogui.press('down',presses=100)\n",
    "        pyautogui.press('down',presses=100)\n",
    "        pyautogui.press('down',presses=100)\n",
    "        pyautogui.press('down',presses=100)\n",
    "  \n",
    "\n",
    "\n",
    "        WebDriverWait(browser, 20).until(\n",
    "            EC.element_to_be_clickable((By.XPATH, '//*[@id=\"main-content\"]/section[2]/button'))).click()\n",
    "\n",
    "    ###################################################################################################\n",
    "    #Write page source to HTML file and download it. \n",
    "    ###################################################################################################\n",
    "\n",
    "    pageSource = browser.page_source\n",
    "\n",
    "    fileToWrite = open(\"LinkedIn_Source.html\", \"w\", encoding=\"utf-8\")\n",
    "    fileToWrite.write(pageSource)\n",
    "\n",
    "    print(\"Downloaded LinkedIn_Source.html file Successfully!\")\n",
    "    fileToWrite.close()\n",
    "\n",
    "    #Create Beautiful soup object\n",
    "    fileToRead = open(\"LinkedIn_Source.html\", \"r\", encoding=\"utf-8\")\n",
    "    soup = BeautifulSoup(fileToRead.read())\n",
    "\n",
    "    JobLinks = []\n",
    "    card_list = soup.find_all('div', {'class': 'base-card relative w-full hover:no-underline focus:no-underline base-card--link base-search-card base-search-card--link job-search-card'})\n",
    "    for card in card_list:\n",
    "        link = card.find('a')['href']\n",
    "        JobLinks.append(link)\n",
    "\n",
    "    print(len(JobLinks))\n",
    "\n",
    "    # Download the HTML files\n",
    "\n",
    "    i=1 # Last index of the HTML file downloaded\n",
    "    for link in JobLinks:\n",
    "        response = requests.get(link,headers = user_agent)\n",
    "        soup1 = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "        with open(\"Linkedin_Job_NEW_[%d].html\"%(i), \"w\", encoding='utf-8') as file:\n",
    "            file.write(soup1.prettify())\n",
    "            file.close()\n",
    "        i=i+1\n",
    "\n",
    "    # Parsing the content\n",
    "\n",
    "    ###################################################################################################\n",
    "    # Defining Empty Lists\n",
    "    Job_Title=[]\n",
    "    CName = []\n",
    "    CUrl = []\n",
    "    Job_Location = []\n",
    "    No_of_Applicants = []\n",
    "\n",
    "    Recruiter_Name = []\n",
    "    Recruiter_Title = []\n",
    "    Recruiter_Url = []\n",
    "\n",
    "    Job_Descriptions=[]\n",
    "    Job_Levels=[]\n",
    "    Job_Types=[]\n",
    "    Job_function =[]\n",
    "    Job_Industry =[]\n",
    "    Similar_Jobs_Links=[]\n",
    "    Date_Downloaded=[]\n",
    "    Date_Posted=[]\n",
    "    ###################################################################################################\n",
    "\n",
    "    for i in range(len(JobLinks)):\n",
    "    #for i in range(940,1038):\n",
    "        with open(\"Linkedin_Job_NEW_[%d].html\"%(i+1), 'r',encoding=\"utf-8\") as file: # Change the file name\n",
    "            html = file.read()\n",
    "            soup_html = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "            #TITLE\n",
    "            if (soup_html.select('#main-content > section.core-rail.mx-auto.papabear\\:w-core-rail-width.mamabear\\:max-w-\\[790px\\].babybear\\:max-w-\\[790px\\] > div > section.top-card-layout.container-lined.overflow-hidden.babybear\\:rounded-\\[0px\\] > div > div.top-card-layout__entity-info-container.flex.flex-wrap.papabear\\:flex-nowrap > div>h1')):\n",
    "                title = soup_html.select('#main-content > section.core-rail.mx-auto.papabear\\:w-core-rail-width.mamabear\\:max-w-\\[790px\\].babybear\\:max-w-\\[790px\\] > div > section.top-card-layout.container-lined.overflow-hidden.babybear\\:rounded-\\[0px\\] > div > div.top-card-layout__entity-info-container.flex.flex-wrap.papabear\\:flex-nowrap > div>h1')\n",
    "                Job_Title.append(title[0].text.strip())\n",
    "            else:\n",
    "                Job_Title.append(None)\n",
    "\n",
    "            #Company name and #Company LinkedIn URL\n",
    "\n",
    "            if (soup_html.select(\"#main-content > section.core-rail.mx-auto.papabear\\:w-core-rail-width.mamabear\\:max-w-\\[790px\\].babybear\\:max-w-\\[790px\\] > div > section.top-card-layout.container-lined.overflow-hidden.babybear\\:rounded-\\[0px\\] > div > div.top-card-layout__entity-info-container.flex.flex-wrap.papabear\\:flex-nowrap > div > h4 > div:nth-child(1) > span:nth-child(1) > a\")):\n",
    "                name = soup_html.select(\"#main-content > section.core-rail.mx-auto.papabear\\:w-core-rail-width.mamabear\\:max-w-\\[790px\\].babybear\\:max-w-\\[790px\\] > div > section.top-card-layout.container-lined.overflow-hidden.babybear\\:rounded-\\[0px\\] > div > div.top-card-layout__entity-info-container.flex.flex-wrap.papabear\\:flex-nowrap > div > h4 > div:nth-child(1) > span:nth-child(1) > a\")\n",
    "                CName.append(name[0].text.strip())\n",
    "                CUrl.append(name[0].get(\"href\"))\n",
    "            else:\n",
    "                CName.append(None)\n",
    "                CUrl.append(None)\n",
    "\n",
    "\n",
    "            # Job location\n",
    "\n",
    "            if (soup_html.select(\"#main-content > section.core-rail.mx-auto.papabear\\:w-core-rail-width.mamabear\\:max-w-\\[790px\\].babybear\\:max-w-\\[790px\\] > div > section.top-card-layout.container-lined.overflow-hidden.babybear\\:rounded-\\[0px\\] > div > div.top-card-layout__entity-info-container.flex.flex-wrap.papabear\\:flex-nowrap > div > h4 > div:nth-child(1) > span.topcard__flavor.topcard__flavor--bullet\")):        \n",
    "                loc = soup_html.select(\"#main-content > section.core-rail.mx-auto.papabear\\:w-core-rail-width.mamabear\\:max-w-\\[790px\\].babybear\\:max-w-\\[790px\\] > div > section.top-card-layout.container-lined.overflow-hidden.babybear\\:rounded-\\[0px\\] > div > div.top-card-layout__entity-info-container.flex.flex-wrap.papabear\\:flex-nowrap > div > h4 > div:nth-child(1) > span.topcard__flavor.topcard__flavor--bullet\")\n",
    "                Job_Location.append(loc[0].text.strip())\n",
    "            else:\n",
    "                 Job_Location.append(None)\n",
    "\n",
    "            # Number of Applicants\n",
    "\n",
    "            if (soup_html.select(\"#main-content > section.core-rail.mx-auto.papabear\\:w-core-rail-width.mamabear\\:max-w-\\[790px\\].babybear\\:max-w-\\[790px\\] > div > section.top-card-layout.container-lined.overflow-hidden.babybear\\:rounded-\\[0px\\] > div > div.top-card-layout__entity-info-container.flex.flex-wrap.papabear\\:flex-nowrap > div > h4 > div:nth-child(2) > figure > figcaption\")):\n",
    "                num = soup_html.select(\"#main-content > section.core-rail.mx-auto.papabear\\:w-core-rail-width.mamabear\\:max-w-\\[790px\\].babybear\\:max-w-\\[790px\\] > div > section.top-card-layout.container-lined.overflow-hidden.babybear\\:rounded-\\[0px\\] > div > div.top-card-layout__entity-info-container.flex.flex-wrap.papabear\\:flex-nowrap > div > h4 > div:nth-child(2) > figure > figcaption\")\n",
    "                No_of_Applicants.append(num[0].text.strip())\n",
    "            else:\n",
    "                No_of_Applicants.append(None)\n",
    "\n",
    "            #RECRUITER DETAILS\n",
    "\n",
    "            # Rec Name\n",
    "            if (soup_html.select(\"#main-content > section.core-rail.mx-auto.papabear\\:w-core-rail-width.mamabear\\:max-w-\\[790px\\].babybear\\:max-w-\\[790px\\] > div > div > section.core-section-container.my-3.description > div > div.message-the-recruiter > div > img\")):\n",
    "\n",
    "                rec_name = soup_html.select(\"#main-content > section.core-rail.mx-auto.papabear\\:w-core-rail-width.mamabear\\:max-w-\\[790px\\].babybear\\:max-w-\\[790px\\] > div > div > section.core-section-container.my-3.description > div > div.message-the-recruiter > div > img\")\n",
    "                Recruiter_Name.append(rec_name[0]['alt'])\n",
    "            else:\n",
    "                Recruiter_Name.append(None)\n",
    "\n",
    "\n",
    "            # Rec Title\n",
    "\n",
    "            if (soup_html.select(\"#main-content > section.core-rail.mx-auto.papabear\\:w-core-rail-width.mamabear\\:max-w-\\[790px\\].babybear\\:max-w-\\[790px\\] > div > div > section.core-section-container.my-3.description > div > div.message-the-recruiter > div > div.base-main-card__info.self-center.ml-1.flex-1.relative.break-words.papabear\\:min-w-0.mamabear\\:min-w-0.babybear\\:w-full > h4\")):\n",
    "                rec_title = soup_html.select(\"#main-content > section.core-rail.mx-auto.papabear\\:w-core-rail-width.mamabear\\:max-w-\\[790px\\].babybear\\:max-w-\\[790px\\] > div > div > section.core-section-container.my-3.description > div > div.message-the-recruiter > div > div.base-main-card__info.self-center.ml-1.flex-1.relative.break-words.papabear\\:min-w-0.mamabear\\:min-w-0.babybear\\:w-full > h4\")\n",
    "                Recruiter_Title.append(rec_title[0].text.strip())\n",
    "            else:\n",
    "                Recruiter_Title.append(None)\n",
    "\n",
    "            #Rec Link\n",
    "            if (soup_html.select(\"#main-content > section.core-rail.mx-auto.papabear\\:w-core-rail-width.mamabear\\:max-w-\\[790px\\].babybear\\:max-w-\\[790px\\] > div > div > section.core-section-container.my-3.description > div > div.message-the-recruiter > div > div.base-main-card__ctas.z-\\[3\\].self-center.ml-3.babybear\\:ml-1.babybear\\:self-start > a\")):\n",
    "                rec_message_link = soup_html.select(\"#main-content > section.core-rail.mx-auto.papabear\\:w-core-rail-width.mamabear\\:max-w-\\[790px\\].babybear\\:max-w-\\[790px\\] > div > div > section.core-section-container.my-3.description > div > div.message-the-recruiter > div > div.base-main-card__ctas.z-\\[3\\].self-center.ml-3.babybear\\:ml-1.babybear\\:self-start > a\")\n",
    "                Recruiter_Url.append(rec_message_link[0].get(\"href\"))\n",
    "            else:\n",
    "                Recruiter_Url.append(None)\n",
    "\n",
    "            # JOB DESCRIPTION\n",
    "\n",
    "            if (soup_html.select(\"#main-content > section.core-rail.mx-auto.papabear\\:w-core-rail-width.mamabear\\:max-w-\\[790px\\].babybear\\:max-w-\\[790px\\] > div > div > section.core-section-container.my-3.description > div > div.description__text.description__text--rich > section > div\")):\n",
    "\n",
    "                job_descr = soup_html.select(\"#main-content > section.core-rail.mx-auto.papabear\\:w-core-rail-width.mamabear\\:max-w-\\[790px\\].babybear\\:max-w-\\[790px\\] > div > div > section.core-section-container.my-3.description > div > div.description__text.description__text--rich > section > div\")\n",
    "                Job_Descriptions.append(job_descr[0].text.strip())\n",
    "            else:\n",
    "                Job_Descriptions.append(None)\n",
    "\n",
    "            # JOB LEVEL\n",
    "\n",
    "            if (soup_html.select(\"#main-content > section.core-rail.mx-auto.papabear\\:w-core-rail-width.mamabear\\:max-w-\\[790px\\].babybear\\:max-w-\\[790px\\] > div > div > section.core-section-container.my-3.description > div > ul > li:nth-child(1) > span\")):\n",
    "                job_level = soup_html.select(\"#main-content > section.core-rail.mx-auto.papabear\\:w-core-rail-width.mamabear\\:max-w-\\[790px\\].babybear\\:max-w-\\[790px\\] > div > div > section.core-section-container.my-3.description > div > ul > li:nth-child(1) > span\")\n",
    "                Job_Levels.append(job_level[0].text.strip())\n",
    "\n",
    "            else:\n",
    "                Job_Levels.append(None)\n",
    "\n",
    "            # JOB TYPE\n",
    "\n",
    "            if (soup_html.select(\"#main-content > section.core-rail.mx-auto.papabear\\:w-core-rail-width.mamabear\\:max-w-\\[790px\\].babybear\\:max-w-\\[790px\\] > div > div > section.core-section-container.my-3.description > div > ul > li:nth-child(2) > span\")):\n",
    "                type_job = soup_html.select(\"#main-content > section.core-rail.mx-auto.papabear\\:w-core-rail-width.mamabear\\:max-w-\\[790px\\].babybear\\:max-w-\\[790px\\] > div > div > section.core-section-container.my-3.description > div > ul > li:nth-child(2) > span\")\n",
    "                Job_Types.append(type_job[0].text.strip())\n",
    "\n",
    "            else:\n",
    "                Job_Types.append(None)\n",
    "\n",
    "            # JOB FUNCTION\n",
    "\n",
    "            if (soup_html.select(\"#main-content > section.core-rail.mx-auto.papabear\\:w-core-rail-width.mamabear\\:max-w-\\[790px\\].babybear\\:max-w-\\[790px\\] > div > div > section.core-section-container.my-3.description > div > ul > li:nth-child(3) > span\")):\n",
    "                function_job = soup_html.select(\"#main-content > section.core-rail.mx-auto.papabear\\:w-core-rail-width.mamabear\\:max-w-\\[790px\\].babybear\\:max-w-\\[790px\\] > div > div > section.core-section-container.my-3.description > div > ul > li:nth-child(3) > span\")\n",
    "                Job_function.append(function_job[0].text.strip())\n",
    "            else:\n",
    "                Job_function.append(None)\n",
    "\n",
    "            # JOB INDUSTRY\n",
    "\n",
    "            if (soup_html.select(\"#main-content > section.core-rail.mx-auto.papabear\\:w-core-rail-width.mamabear\\:max-w-\\[790px\\].babybear\\:max-w-\\[790px\\] > div > div > section.core-section-container.my-3.description > div > ul > li:nth-child(4) > span\")):\n",
    "                industry_job = soup_html.select(\"#main-content > section.core-rail.mx-auto.papabear\\:w-core-rail-width.mamabear\\:max-w-\\[790px\\].babybear\\:max-w-\\[790px\\] > div > div > section.core-section-container.my-3.description > div > ul > li:nth-child(4) > span\")\n",
    "                Job_Industry.append(industry_job[0].text.strip())\n",
    "            else:\n",
    "                Job_Industry.append(None)\n",
    "\n",
    "            # SIMILAR JOB LINKS\n",
    "\n",
    "            if (soup_html.find_all('a', {'class': 'base-card__full-link absolute top-0 right-0 bottom-0 left-0 p-0 z-[2]'})):\n",
    "                similar_jobs_cards = soup_html.find_all('a', {'class': 'base-card__full-link absolute top-0 right-0 bottom-0 left-0 p-0 z-[2]'})\n",
    "\n",
    "                Similar_Jobs_Links_for_this_job =[]\n",
    "                for card in similar_jobs_cards:\n",
    "                    sim_link = card.get('href')\n",
    "                    Similar_Jobs_Links_for_this_job.append(sim_link)\n",
    "\n",
    "                Similar_Jobs_Links.append(Similar_Jobs_Links_for_this_job)\n",
    "            else:\n",
    "                Similar_Jobs_Links.append(None)\n",
    "\n",
    "            # DATE DOWNLOADED\n",
    "            today = str(datetime.date.today())\n",
    "            Date_Downloaded.append(today)\n",
    "\n",
    "            # DATE POSTED\n",
    "\n",
    "            if (soup_html.select(\"#main-content > section.core-rail.mx-auto.papabear\\:w-core-rail-width.mamabear\\:max-w-\\[790px\\].babybear\\:max-w-\\[790px\\] > div > section.top-card-layout.container-lined.overflow-hidden.babybear\\:rounded-\\[0px\\] > div > div.top-card-layout__entity-info-container.flex.flex-wrap.papabear\\:flex-nowrap > div > h4 > div:nth-child(2) > span\")):\n",
    "                posted = soup_html.select(\"#main-content > section.core-rail.mx-auto.papabear\\:w-core-rail-width.mamabear\\:max-w-\\[790px\\].babybear\\:max-w-\\[790px\\] > div > section.top-card-layout.container-lined.overflow-hidden.babybear\\:rounded-\\[0px\\] > div > div.top-card-layout__entity-info-container.flex.flex-wrap.papabear\\:flex-nowrap > div > h4 > div:nth-child(2) > span\")\n",
    "                Date_Posted.append(posted[0].text.strip())\n",
    "            else:\n",
    "                Date_Posted.append(None)\n",
    "\n",
    "    # ################################################################################################################\n",
    "\n",
    "    print(\"JOB TITLES\")\n",
    "    print(Job_Title)\n",
    "    print(\"\\n\")\n",
    "\n",
    "    print(\"COMPANY NAMES\")\n",
    "    print(CName)\n",
    "    print(\"\\n\")\n",
    "\n",
    "    print(\"COMPANY URLS\")\n",
    "    print(CUrl)\n",
    "    print(\"\\n\")\n",
    "\n",
    "    print(\"JOB LOCATIONS\")\n",
    "    print(Job_Location)\n",
    "    print(\"\\n\")\n",
    "\n",
    "    print(\"# APPLICANTS\")\n",
    "    print(No_of_Applicants)\n",
    "    print(\"\\n\")\n",
    "\n",
    "    print(\"#DATE POSTED\")\n",
    "    print(Date_Posted)\n",
    "    print(\"\\n\")\n",
    "\n",
    "    print(\"DATE DOWNLOADED\")\n",
    "    print(Date_Downloaded)\n",
    "    print(\"\\n\")\n",
    "\n",
    "    # ################################################################################################################\n",
    "\n",
    "    print(\"RECRUITER NAME\")\n",
    "    print(Recruiter_Name)\n",
    "    print(\"\\n\")\n",
    "\n",
    "    print(\"RECRUITER TITLE\")\n",
    "    print(Recruiter_Title)\n",
    "    print(\"\\n\")\n",
    "\n",
    "    print(\"RECRUITER LINKEDIN URL\")\n",
    "    print(Recruiter_Url)\n",
    "    print(\"\\n\")\n",
    "\n",
    "    # ################################################################################################################\n",
    "\n",
    "    print(\"JOB DESCRIPTIONS\")\n",
    "    print(Job_Descriptions)\n",
    "    print(\"\\n\")\n",
    "\n",
    "    print(\"JOB LEVELS\")\n",
    "    print(Job_Levels)\n",
    "    print(\"\\n\")\n",
    "\n",
    "    print(\"JOB TYPES\")\n",
    "    print(Job_Types)\n",
    "    print(\"\\n\")\n",
    "\n",
    "    print(\"JOB FUNCTIONS\")\n",
    "    print(Job_function)\n",
    "    print(\"\\n\")\n",
    "\n",
    "    print(\"JOB INDUSTRY\")\n",
    "    print(Job_Industry)\n",
    "    print(\"\\n\")\n",
    "\n",
    "    print(\"SIMILAR JOBS\")\n",
    "    print(Similar_Jobs_Links) \n",
    "    print(\"\\n\")   \n",
    "\n",
    "    # ################################################################################################################\n",
    "\n",
    "    # Connecting to Mongo DB\n",
    "\n",
    "    import pymongo\n",
    "    client = pymongo.MongoClient(\"mongodb://localhost:27017/\")\n",
    "\n",
    "    db = client[\"LinkedIn_DB\"]\n",
    "\n",
    "    collection1 = db[\"JOBS_Basic\"]\n",
    "    collection2 = db[\"JOBS_Recruiter\"]\n",
    "    collection3 = db[\"JOBS_DESCRIPTION\"]\n",
    "    collection4 = db[\"JOBS_Advanced\"]\n",
    "\n",
    "    # Inserting into the MongoDB\n",
    "\n",
    "    import datetime\n",
    "\n",
    "    x=0\n",
    "    for i in range(len(JobLinks)-1):\n",
    "\n",
    "        data1 = {\n",
    "                    \"ID\": x+1,\n",
    "                    \"TITLE\": Job_Title[i],\n",
    "                    \"COMPANY\": CName[i],\n",
    "                    \"LOCATION\": Job_Location[i],\n",
    "                    \"#Applicants\": No_of_Applicants[i],\n",
    "                    \"Date Posted\": Date_Posted[i],\n",
    "                    \"Job URL\": JobLinks[i],\n",
    "                    \"Company URL\": CUrl[i],\n",
    "                    \"Date_Downloaded\" : Date_Downloaded[i]                \n",
    "                }\n",
    "\n",
    "        data2 = {\n",
    "                    \"ID\": x+1,\n",
    "                    \"Recruiter Name\": Recruiter_Name[i],\n",
    "                    \"Recruiter Title\": Recruiter_Title[i],\n",
    "                    \"Recruiter URL\": Recruiter_Url[i],\n",
    "                    \"Date_Downloaded\" : Date_Downloaded[i] \n",
    "                }\n",
    "        data3 = {\n",
    "                    \"ID\": x+1,\n",
    "                    \"DESCRIPTIONS\": Job_Descriptions[i],\n",
    "                    \"Date_Downloaded\" : Date_Downloaded[i] \n",
    "                }\n",
    "        data4 = {\n",
    "                    \"ID\": x+1,\n",
    "                    \"LEVEL\": Job_Levels[i],\n",
    "                    \"TYPE\": Job_Types[i],\n",
    "                    \"FUNCTION\": Job_function[i],\n",
    "                    \"INDUSTRY\": Job_Industry[i],\n",
    "                    \"SIMILAR JOBS\": Similar_Jobs_Links[i] ,\n",
    "                    \"Date_Downloaded\" : Date_Downloaded[i] \n",
    "                }\n",
    "\n",
    "        collection1.insert_one(data1)\n",
    "        collection2.insert_one(data2)\n",
    "        collection3.insert_one(data3)\n",
    "        collection4.insert_one(data4)\n",
    "\n",
    "        print(\"Job [%d] Details inserted into MongoDB successfully!\"%(x+1))\n",
    "        print(\"/n\")\n",
    "        x=x+1\n",
    "\n",
    "    print(x)\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3ef4ccbf",
   "metadata": {},
   "source": [
    "# PERSONALIZED JOB FILTERING"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6ad37f4f",
   "metadata": {},
   "source": [
    "### ENTER TITLE/ KEYWORD/ LOCATION/ TYPE/ INDUSTRY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5103f2bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Filtering(job_type1,keyword,location,job_type2,industry): \n",
    "    import pymongo\n",
    "    client = pymongo.MongoClient(\"mongodb://localhost:27017/\")\n",
    "\n",
    "    db = client[\"LinkedIn_DB\"]\n",
    "\n",
    "    collection1 = db[\"JOBS_Basic\"]\n",
    "    collection2 = db[\"JOBS_Recruiter\"]\n",
    "    collection3 = db[\"JOBS_DESCRIPTION\"]\n",
    "    collection4 = db[\"JOBS_Advanced\"]\n",
    "\n",
    "    user_input1 = job_type1.value\n",
    "    user_input5 = keyword.value\n",
    "    user_input2 = location.value\n",
    "    user_input3 = job_type2.value\n",
    "    user_input4 = industry.value\n",
    "\n",
    "    # print(user_input1)\n",
    "    # print(user_input5)\n",
    "    # print(user_input2)\n",
    "    # print(user_input3)\n",
    "    # print(user_input4)\n",
    "\n",
    "    ####################################################################################################\n",
    "\n",
    "    # Filter collection1 based on user_input1 and user_input2\n",
    "    if user_input1:\n",
    "        filter_1 = {'TITLE': {'$regex': '.*' + user_input1 + '.*', '$options': 'i'}}\n",
    "    else:\n",
    "        filter_1 = {}\n",
    "\n",
    "    if user_input2:\n",
    "        filter_2 = {'LOCATION': {'$regex': '.*' + user_input2 + '.*'}}\n",
    "    else:\n",
    "        filter_2 = {}\n",
    "\n",
    "    filtered_docs_1 = collection1.find({'$and': [filter_1, filter_2]})\n",
    "    #print(collection1.count_documents({'$and': [filter_1, filter_2]}))\n",
    "\n",
    "    ####################################################################################################\n",
    "    # Filter collection4 based on user_input3 and user_input4\n",
    "    if user_input3:\n",
    "        filter_3 = {'TYPE': {'$regex': '.*' + user_input3 + '.*'}}\n",
    "    else:\n",
    "        filter_3 = {}\n",
    "\n",
    "    if user_input4:\n",
    "        filter_4 = {'INDUSTRY': {'$regex': '.*' + user_input4 + '.*'}}\n",
    "    else:\n",
    "        filter_4 = {}\n",
    "\n",
    "    filtered_docs_4 = collection4.find({'$and': [filter_3, filter_4]})\n",
    "    #print(collection4.count_documents({'$and': [filter_3, filter_4]}))\n",
    "    ####################################################################################################\n",
    "\n",
    "    # Filter collection3 based on user_input5\n",
    "    if user_input5:\n",
    "        filter_5 = {'DESCRIPTIONS': {'$regex': '.*' + user_input5 + '.*'}}\n",
    "    else:\n",
    "        filter_5 = {}\n",
    "\n",
    "    filtered_docs_5 = collection3.find(filter_5)\n",
    "    #print(collection3.count_documents(filter_5))\n",
    "\n",
    "    ####################################################################################################\n",
    "\n",
    "    # Get the IDs from filtered_docs_1\n",
    "    ids_1 = [doc['ID'] for doc in filtered_docs_1]\n",
    "\n",
    "    # Get the IDs from filtered_docs_4\n",
    "    ids_4 = [doc['ID'] for doc in filtered_docs_4]\n",
    "\n",
    "    # Get the IDs from filtered_docs_5\n",
    "    ids_5 = [doc['ID'] for doc in filtered_docs_5]\n",
    "    ####################################################################################################\n",
    "\n",
    "    # Find the common IDs\n",
    "\n",
    "    common_ids = set(ids_1).intersection(set(ids_4)).intersection(set(ids_5))\n",
    "\n",
    "    n6= collection1.count_documents({'ID': {'$in': list(common_ids)}})\n",
    "    print(\"Found %d jobs!\"%n6)\n",
    "    ####################################################################################################\n",
    "\n",
    "    JOBS6=[]\n",
    "    # Print the jobs with common IDs\n",
    "    for doc in collection1.find({'ID': {'$in': list(common_ids)}}):\n",
    "        JOBS6.append([doc['ID'], doc['TITLE'], doc['COMPANY'], doc['LOCATION'], doc['#Applicants'],doc['Date Posted'],doc['Job URL'],doc['Company URL'],doc['Date_Downloaded'] ])\n",
    "\n",
    "    if(filter_1==filter_2==filter_3==filter_4==filter_5=={}):\n",
    "        print('')\n",
    "    else:\n",
    "        df6 = pd.DataFrame(JOBS6, columns=['ID', 'TITLE', 'COMPANY', 'LOCATION', '#Applicants', 'Date Posted', 'Job URL', 'Company URL', 'Date_Downloaded'])\n",
    "        print(df6)\n",
    "        df6.to_csv('filtered_jobs.csv', index=False)\n",
    "\n",
    "\n",
    "    ####################################################################################################\n",
    "    # DOWNLOAD THE FILTERED JOBS AS CSV\n",
    "     ####################################################################################################\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "756f27f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Generate_Form():\n",
    "    job_type1 = widgets.Text(\n",
    "        value='',\n",
    "        placeholder='Data Scientist/Data Analyst/Engineer/Product Manager', \n",
    "        description='What Job are you looking for?',\n",
    "        disabled=False,\n",
    "        layout={'width': '100%'},\n",
    "        style={'description_width': '300px'}\n",
    "    )\n",
    "\n",
    "    keyword = widgets.Text(\n",
    "        value='',\n",
    "        placeholder='SQL/Python/Mathematics/Computer Science/AI/neural networks',\n",
    "        description='Enter the keyword you are looking for in the Job Description:',\n",
    "        disabled=False,\n",
    "        layout={'width': '100%'},\n",
    "        style={'description_width': '300px'}\n",
    "    )\n",
    "\n",
    "    location = widgets.Text(\n",
    "        value='',\n",
    "        placeholder='banglore/delhi/noida/hyderabad',\n",
    "        description='Which Location?',\n",
    "        disabled=False,\n",
    "        layout={'width': '100%'},\n",
    "        style={'description_width': '300px'}\n",
    "    )\n",
    "\n",
    "    job_type2 = widgets.Text(\n",
    "        value='',\n",
    "        placeholder='Full-time/Internship/Contract',\n",
    "        description='What type of job are you looking for?',\n",
    "        disabled=False,\n",
    "        layout={'width': '100%'},\n",
    "        style={'description_width': '300px'}\n",
    "    )\n",
    "\n",
    "    industry = widgets.Text(\n",
    "        value='',\n",
    "        placeholder='Software/Technology/Health Care/Music',\n",
    "        description='Which Industry do you prefer?',\n",
    "        disabled=False,\n",
    "        layout={'width': '100%'},\n",
    "        style={'description_width': '300px'}\n",
    "    )\n",
    "\n",
    "    button = widgets.Button(description='Submit')\n",
    "\n",
    "    def on_button_clicked(b):\n",
    "        print('What Job are you looking for?', job_type1.value)\n",
    "        print('Enter the keyword you are looking for:', job_type1.value)\n",
    "        print('Which Location?', location.value)\n",
    "        print('What type of job are you looking for?', job_type2.value)\n",
    "        print('Which Industry do you prefer?', industry.value)\n",
    "        #print(f\"Filtering results for {job_type1.value} with keyword '{keyword_text}' in {location_text} for a {job_type_text} job in the {industry_text} industry.\")\n",
    "\n",
    "        \n",
    "        Filtering(job_type1,keyword,location,job_type2,industry)\n",
    "\n",
    "\n",
    "    button.on_click(on_button_clicked)\n",
    "\n",
    "    form = widgets.VBox([job_type1, keyword, location, job_type2, industry, button])\n",
    "    display(form)\n",
    "    \n",
    "    return job_type1,keyword,location,job_type2,industry\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "54d76c8c",
   "metadata": {},
   "source": [
    "# FUTURE SCOPE: SALARY FILTER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "03219015",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Future_Salary_Filter():\n",
    "    import pymongo\n",
    "    client = pymongo.MongoClient(\"mongodb://localhost:27017/\")\n",
    "\n",
    "    db = client[\"LinkedIn_DB\"]\n",
    "\n",
    "    collection1 = db[\"JOBS_Basic\"]\n",
    "    collection2 = db[\"JOBS_Recruiter\"]\n",
    "    collection3 = db[\"JOBS_DESCRIPTION\"]\n",
    "    collection4 = db[\"JOBS_Advanced\"]\n",
    "    \n",
    "    for document in collection3.find():\n",
    "        description = str(document.get(\"DESCRIPTIONS\", \"\"))\n",
    "\n",
    "        match = re.search(r\"\\$(\\d{1,3}(?:,\\d{3})*)(?:\\D+)?\\$(\\d{1,3}(?:,\\d{3})*)\", description)\n",
    "        lower_salary = match.group(1).replace(\",\", \"\") if match else None\n",
    "        upper_salary = match.group(2).replace(\",\", \"\") if match else None\n",
    "        lower_salary= float(lower_salary) if lower_salary else None\n",
    "        upper_salary= float(upper_salary) if lower_salary else None\n",
    "\n",
    "        # update the document with the new columns\n",
    "        collection3.update_one({\"_id\": document[\"_id\"]}, {\"$set\": {\"lower_salary\": lower_salary, \"upper_salary\": upper_salary}})\n",
    "\n",
    "    print(\"The current approach of using regex to add salary ranges to the Jobs_Description collection has resulted in inaccurate values because it includes both yearly and hourly pay rates. To address this issue in the future, a more precise regex function could be developed that can handle different salary formats, such as base pay, hourly pay rate, and upper and lower limits for annual pay rates. The extracted values could be converted uniformly to hourly or annual pay rates and stored in the database. This enhancement would enable the search filter to match user input values with job offerings based on their expected salary.\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a5d48450",
   "metadata": {},
   "source": [
    "# MAIN FUNCTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0261d288",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You entered: developer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SAURABH MISHRA\\AppData\\Local\\Temp\\ipykernel_13856\\964060498.py:10: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  browser = webdriver.Chrome(executable_path=\"/usr/local/bin/chromedriver.exe\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded LinkedIn_Source.html file Successfully!\n",
      "174\n",
      "Error: HTTPSConnectionPool(host='in.linkedin.com', port=443): Max retries exceeded with url: /jobs/view/web-developer-at-fortanix-3538127613?refId=kA9%2FLLZFho1ToKBGhW%2Barw%3D%3D&trackingId=vkbcbTDcrsHyvOqcupZJ7w%3D%3D&position=21&pageNum=3&trk=public_jobs_jserp-result_search-card (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x0000020F8D5C6690>, 'Connection to in.linkedin.com timed out. (connect timeout=None)'))\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    try: \n",
    "        import pymongo\n",
    "        client = pymongo.MongoClient(\"mongodb://localhost:27017/\")\n",
    "\n",
    "        db = client[\"LinkedIn_DB\"]\n",
    "\n",
    "        collection1 = db[\"JOBS_Basic\"]\n",
    "        collection2 = db[\"JOBS_Recruiter\"]\n",
    "        collection3 = db[\"JOBS_DESCRIPTION\"]\n",
    "        collection4 = db[\"JOBS_Advanced\"]\n",
    "        #####################################################################################################\n",
    "        #Running this will ask you to enter a job title, through a Selenium browser will open the LinkedIn page\n",
    "        #and scroll for jobs and inserts the job details in the database\n",
    "        #####################################################################################################\n",
    "        \n",
    "        ### FIRST FUNCTION CALL ###\n",
    "        ScrapingToMongoDb()\n",
    "        \n",
    "        #####################################################################################################\n",
    "        #Generates a Personalized form asking you to input values\n",
    "        #####################################################################################################\n",
    "        \n",
    "        ### SECOND FUNCTION CALL ###\n",
    "        # job_type1,keyword,location,job_type2,industry = Generate_Form()\n",
    "        \n",
    "        \n",
    "        #####################################################################################################\n",
    "        # Call filtering function with the user inputs.Filters the MongoDB database and downloads the csv file\n",
    "        #####################################################################################################\n",
    "        ### THIRD FUNCTION CALL ###\n",
    "        \n",
    "        # Filtering(job_type1,keyword,location,job_type2,industry)\n",
    "        \n",
    "        \n",
    "        \n",
    "        #####################################################################################################\n",
    "        #This function is for the the additional feature that can be added in the future to query the database for salary ranges\n",
    "        #####################################################################################################\n",
    "        \n",
    "        ### FOURTH FUNCTION CALL ###\n",
    "        #Future_Salary_Filter()\n",
    "        \n",
    "        #####################################################################################################\n",
    "\n",
    "    except Exception as ex:\n",
    "        print('Error: ' + str(ex))\n",
    "\n",
    "        \n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7c24bb55",
   "metadata": {},
   "source": [
    "# ADDITIONAL FILTERING ILLUSTRATIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "67a41c7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymongo\n",
    "client = pymongo.MongoClient(\"mongodb://localhost:27017/\")\n",
    "\n",
    "db = client[\"LinkedIn_DB\"]\n",
    "\n",
    "collection1 = db[\"JOBS_Basic\"]\n",
    "collection2 = db[\"JOBS_Recruiter\"]\n",
    "collection3 = db[\"JOBS_DESCRIPTION\"]\n",
    "collection4 = db[\"JOBS_Advanced\"]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "67f5d47e",
   "metadata": {},
   "source": [
    "Printing all the basic Job Details from Collection 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "08be49fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     ID                 TITLE                               COMPANY  \\\n",
      "0     1                  None                                  None   \n",
      "1     2     Software Engineer                                  Nike   \n",
      "2     3  Software Developer 1                                Oracle   \n",
      "3     4  Software Developer 1                                Oracle   \n",
      "4     5  Software Developer 1                                Oracle   \n",
      "..   ..                   ...                                   ...   \n",
      "181  19      Python Developer       Mspire Ventures Private Limited   \n",
      "182  20      Python Developer  Sinewave Computer Services Pvt. Ltd.   \n",
      "183  21      Python Developer                                 Geexu   \n",
      "184  22      Python Developer               Meslova Systems Pvt Ltd   \n",
      "185  23      Python Developer                   NuStar Technologies   \n",
      "\n",
      "                              LOCATION                       #Applicants  \\\n",
      "0                                 None                              None   \n",
      "1                        Beaverton, OR               Over 200 applicants   \n",
      "2                        United States                              None   \n",
      "3                        United States                              None   \n",
      "4                        United States                              None   \n",
      "..                                 ...                               ...   \n",
      "181          Ahmedabad, Gujarat, India                              None   \n",
      "182           Pune, Maharashtra, India  Be among the first 25 applicants   \n",
      "183           Pune, Maharashtra, India               Over 200 applicants   \n",
      "184  Bengaluru North, Karnataka, India                              None   \n",
      "185  Bangalore Urban, Karnataka, India                              None   \n",
      "\n",
      "     Date Posted                                            Job URL  \\\n",
      "0           None  https://www.linkedin.com/jobs/view/jr-software...   \n",
      "1     1 week ago  https://www.linkedin.com/jobs/view/software-en...   \n",
      "2    5 hours ago  https://www.linkedin.com/jobs/view/software-de...   \n",
      "3    5 hours ago  https://www.linkedin.com/jobs/view/software-de...   \n",
      "4    5 hours ago  https://www.linkedin.com/jobs/view/software-de...   \n",
      "..           ...                                                ...   \n",
      "181  1 month ago  https://in.linkedin.com/jobs/view/python-devel...   \n",
      "182  1 month ago  https://in.linkedin.com/jobs/view/python-devel...   \n",
      "183  1 month ago  https://in.linkedin.com/jobs/view/python-devel...   \n",
      "184  1 month ago  https://in.linkedin.com/jobs/view/python-devel...   \n",
      "185  1 month ago  https://in.linkedin.com/jobs/view/python-devel...   \n",
      "\n",
      "                                           Company URL Date_Downloaded  \n",
      "0                                                 None      2023-04-09  \n",
      "1    https://www.linkedin.com/company/nike?trk=publ...      2023-04-09  \n",
      "2    https://www.linkedin.com/company/oracle?trk=pu...      2023-04-09  \n",
      "3    https://www.linkedin.com/company/oracle?trk=pu...      2023-04-09  \n",
      "4    https://www.linkedin.com/company/oracle?trk=pu...      2023-04-09  \n",
      "..                                                 ...             ...  \n",
      "181  https://in.linkedin.com/company/mspirehr?trk=p...      2023-04-09  \n",
      "182  https://in.linkedin.com/company/sinewaveindia?...      2023-04-09  \n",
      "183  https://in.linkedin.com/company/geexu?trk=publ...      2023-04-09  \n",
      "184  https://in.linkedin.com/company/meslova?trk=pu...      2023-04-09  \n",
      "185  https://www.linkedin.com/company/nustar-techno...      2023-04-09  \n",
      "\n",
      "[186 rows x 9 columns]\n"
     ]
    }
   ],
   "source": [
    "JOBS1 = []\n",
    "\n",
    "for job in collection1.find():\n",
    "    JOBS1.append([job['ID'], job['TITLE'], job['COMPANY'], job['LOCATION'], job['#Applicants'],job['Date Posted'],job['Job URL'],job['Company URL'],job['Date_Downloaded'] ])\n",
    "\n",
    "df1 = pd.DataFrame(JOBS1, columns=['ID', 'TITLE', 'COMPANY', 'LOCATION', '#Applicants', 'Date Posted', 'Job URL', 'Company URL', 'Date_Downloaded'])\n",
    "print(df1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6541f321",
   "metadata": {},
   "source": [
    "### 1. By Location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f6674481",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 65 jobs!\n",
      "    ID                                   TITLE  \\\n",
      "0    2                Junior Software Engineer   \n",
      "1    3  Software Engineer (Entry Level)_Remote   \n",
      "2    4                       Software Engineer   \n",
      "3    5        Jr Java Script / HTML5 Developer   \n",
      "4    6                  Jr. Software Developer   \n",
      "..  ..                                     ...   \n",
      "60  19                        Python Developer   \n",
      "61  20                        Python Developer   \n",
      "62  21                        Python Developer   \n",
      "63  22                        Python Developer   \n",
      "64  23                        Python Developer   \n",
      "\n",
      "                                 COMPANY                           LOCATION  \\\n",
      "0                              TargetBay         Chennai, Tamil Nadu, India   \n",
      "1                         eStaffing Inc.                              India   \n",
      "2                     JIVA InfoTech Inc.        Hyderabad, Telangana, India   \n",
      "3                             MAPSystems        Bengaluru, Karnataka, India   \n",
      "4                              Helloapps  Bengaluru North, Karnataka, India   \n",
      "..                                   ...                                ...   \n",
      "60       Mspire Ventures Private Limited          Ahmedabad, Gujarat, India   \n",
      "61  Sinewave Computer Services Pvt. Ltd.           Pune, Maharashtra, India   \n",
      "62                                 Geexu           Pune, Maharashtra, India   \n",
      "63               Meslova Systems Pvt Ltd  Bengaluru North, Karnataka, India   \n",
      "64                   NuStar Technologies  Bangalore Urban, Karnataka, India   \n",
      "\n",
      "                         #Applicants  Date Posted  \\\n",
      "0                Over 200 applicants  3 weeks ago   \n",
      "1                Over 200 applicants  3 weeks ago   \n",
      "2   Be among the first 25 applicants  5 hours ago   \n",
      "3   Be among the first 25 applicants  1 month ago   \n",
      "4                               None  1 month ago   \n",
      "..                               ...          ...   \n",
      "60                              None  1 month ago   \n",
      "61  Be among the first 25 applicants  1 month ago   \n",
      "62               Over 200 applicants  1 month ago   \n",
      "63                              None  1 month ago   \n",
      "64                              None  1 month ago   \n",
      "\n",
      "                                              Job URL  \\\n",
      "0   https://in.linkedin.com/jobs/view/junior-softw...   \n",
      "1   https://in.linkedin.com/jobs/view/software-eng...   \n",
      "2   https://in.linkedin.com/jobs/view/software-eng...   \n",
      "3   https://in.linkedin.com/jobs/view/jr-java-scri...   \n",
      "4   https://in.linkedin.com/jobs/view/jr-software-...   \n",
      "..                                                ...   \n",
      "60  https://in.linkedin.com/jobs/view/python-devel...   \n",
      "61  https://in.linkedin.com/jobs/view/python-devel...   \n",
      "62  https://in.linkedin.com/jobs/view/python-devel...   \n",
      "63  https://in.linkedin.com/jobs/view/python-devel...   \n",
      "64  https://in.linkedin.com/jobs/view/python-devel...   \n",
      "\n",
      "                                          Company URL Date_Downloaded  \n",
      "0   https://www.linkedin.com/company/targetbay?trk...      2023-04-09  \n",
      "1   https://www.linkedin.com/company/estaffinginc?...      2023-04-09  \n",
      "2   https://in.linkedin.com/company/jiva-infotech?...      2023-04-09  \n",
      "3   https://in.linkedin.com/company/mapsystemsindi...      2023-04-09  \n",
      "4   https://in.linkedin.com/company/helloapps?trk=...      2023-04-09  \n",
      "..                                                ...             ...  \n",
      "60  https://in.linkedin.com/company/mspirehr?trk=p...      2023-04-09  \n",
      "61  https://in.linkedin.com/company/sinewaveindia?...      2023-04-09  \n",
      "62  https://in.linkedin.com/company/geexu?trk=publ...      2023-04-09  \n",
      "63  https://in.linkedin.com/company/meslova?trk=pu...      2023-04-09  \n",
      "64  https://www.linkedin.com/company/nustar-techno...      2023-04-09  \n",
      "\n",
      "[65 rows x 9 columns]\n"
     ]
    }
   ],
   "source": [
    "# Filter jobs that contain 'India' in the 'LOCATION' field\n",
    "filtered_jobs1 = collection1.find({'LOCATION': {'$regex': '.*India.*'}})\n",
    "\n",
    "n = collection1.count_documents({'LOCATION': {'$regex': '.*India.*'}})\n",
    "JOBS1 = []\n",
    "print(\"Found %d jobs!\"%n)\n",
    "# Print the results\n",
    "for job in filtered_jobs1:\n",
    "    JOBS1.append([job['ID'], job['TITLE'], job['COMPANY'], job['LOCATION'], job['#Applicants'],job['Date Posted'],job['Job URL'],job['Company URL'],job['Date_Downloaded'] ])\n",
    "\n",
    "df2 = pd.DataFrame(JOBS1, columns=['ID', 'TITLE', 'COMPANY', 'LOCATION', '#Applicants', 'Date Posted', 'Job URL', 'Company URL', 'Date_Downloaded'])\n",
    "print(df2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4b71cd77",
   "metadata": {},
   "source": [
    "### 2. By Skills from Description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d87b6ca8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 93 jobs!\n"
     ]
    }
   ],
   "source": [
    "# Find all documents in the JOBS_DESCRIPTION collection that contain \"sql\" or \"python\" skills\n",
    "filtered_docs1 = collection3.find({'$or': [{'DESCRIPTIONS': {'$regex': '.*sql.*', '$options': 'i'}}, \n",
    "                                          {'DESCRIPTIONS': {'$regex': '.*python.*', '$options': 'i'}}\n",
    "                                          ]})\n",
    "\n",
    "N = collection3.count_documents({'$or': [{'DESCRIPTIONS': {'$regex': '.*sql.*', '$options': 'i'}}, \n",
    "                                          {'DESCRIPTIONS': {'$regex': '.*python.*', '$options': 'i'}}\n",
    "                                          ]})\n",
    "\n",
    "print(\"Found %d jobs!\"%N)\n",
    "\n",
    "ID1 = [doc['ID'] for doc in filtered_docs1]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "667fe125",
   "metadata": {},
   "source": [
    "##### From the previous filtered skills, now filter for Data Scientist jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bf86cbd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 0 jobs!\n",
      "Empty DataFrame\n",
      "Columns: [ID, TITLE, COMPANY, LOCATION, #Applicants, Date Posted, Job URL, Company URL, Date_Downloaded]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "# Find all documents in the JOBS_Basic collection that have job title \"data scientist\" and job ID from the previous filtered list\n",
    "filtered_jobs = collection1.find({'$and': [{'TITLE': {'$regex': '.*data scientist.*', '$options': 'i'}},\n",
    "                                            {'ID': {'$in': ID1}}]})\n",
    "\n",
    "N1 = collection1.count_documents({'$and': [{'TITLE': {'$regex': '.*data scientist.*', '$options': 'i'}},\n",
    "                                            {'ID': {'$in': ID1}}]})\n",
    "\n",
    "print(\"Found %d jobs!\"%N1)\n",
    "\n",
    "# Print the filtered jobs\n",
    "JOBS3=[]\n",
    "for job in filtered_jobs:\n",
    "    JOBS3.append([job['ID'], job['TITLE'], job['COMPANY'], job['LOCATION'], job['#Applicants'],job['Date Posted'],job['Job URL'],job['Company URL'],job['Date_Downloaded'] ])\n",
    "\n",
    "df3 = pd.DataFrame(JOBS3, columns=['ID', 'TITLE', 'COMPANY', 'LOCATION', '#Applicants', 'Date Posted', 'Job URL', 'Company URL', 'Date_Downloaded'])\n",
    "print(df3)\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a3bb7988",
   "metadata": {},
   "source": [
    "### 3. By Job Level, Type and Industry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9aca29eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 11 jobs!\n"
     ]
    }
   ],
   "source": [
    "# Find all documents in the JOBS_ADVANCED collection that have \n",
    "#LEVEL containing \"Entry\", TYPE containing \"Full-time\", and INDUSTRY containing \"Technology\"\n",
    "\n",
    "filtered_docs2 = collection4.find({'$and': [{'LEVEL': {'$regex': '.*Entry.*', '$options': 'i'}},\n",
    "                                            {'TYPE': {'$regex': '.*Full-time.*', '$options': 'i'}},\n",
    "                                            {'INDUSTRY': {'$regex': '.*Technology.*', '$options': 'i'}}]})\n",
    "\n",
    "n2 = collection4.count_documents({'$and': [{'LEVEL': {'$regex': '.*Entry.*', '$options': 'i'}},\n",
    "                                            {'TYPE': {'$regex': '.*Full-time.*', '$options': 'i'}},\n",
    "                                            {'INDUSTRY': {'$regex': '.*Technology.*', '$options': 'i'}}]})\n",
    "print(\"Found %d jobs!\"%n2)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f8da7092",
   "metadata": {},
   "source": [
    "##### Find Recruiter URLS for the previous filtered jobs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5c0bb98c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here are the Recruiter profiles!\n",
      "    ID Recruiter Name Recruiter Title Recruiter URL Date_Downloaded\n",
      "0    2           None            None          None      2023-04-09\n",
      "1    8           None            None          None      2023-04-09\n",
      "2   13           None            None          None      2023-04-09\n",
      "3   16           None            None          None      2023-04-09\n",
      "4   18           None            None          None      2023-04-09\n",
      "5   19           None            None          None      2023-04-09\n",
      "6   22           None            None          None      2023-04-09\n",
      "7    2           None            None          None      2023-04-09\n",
      "8    8           None            None          None      2023-04-09\n",
      "9   13           None            None          None      2023-04-09\n",
      "10  16           None            None          None      2023-04-09\n",
      "11  18           None            None          None      2023-04-09\n",
      "12  19           None            None          None      2023-04-09\n",
      "13  22           None            None          None      2023-04-09\n",
      "14   2           None            None          None      2023-04-09\n",
      "15   8           None            None          None      2023-04-09\n",
      "16  13           None            None          None      2023-04-09\n",
      "17  16           None            None          None      2023-04-09\n",
      "18  18           None            None          None      2023-04-09\n",
      "19  19           None            None          None      2023-04-09\n",
      "20  22           None            None          None      2023-04-09\n",
      "21   2           None            None          None      2023-04-09\n",
      "22   8           None            None          None      2023-04-09\n",
      "23  13           None            None          None      2023-04-09\n",
      "24  16           None            None          None      2023-04-09\n",
      "25  18           None            None          None      2023-04-09\n",
      "26  19           None            None          None      2023-04-09\n",
      "27  22           None            None          None      2023-04-09\n",
      "28  43           None            None          None      2023-04-09\n",
      "29  45           None            None          None      2023-04-09\n",
      "30   2           None            None          None      2023-04-09\n",
      "31   8           None            None          None      2023-04-09\n",
      "32  13           None            None          None      2023-04-09\n",
      "33  16           None            None          None      2023-04-09\n",
      "34  18           None            None          None      2023-04-09\n",
      "35  19           None            None          None      2023-04-09\n",
      "36  22           None            None          None      2023-04-09\n",
      "37   2           None            None          None      2023-04-09\n",
      "38   8           None            None          None      2023-04-09\n",
      "39  13           None            None          None      2023-04-09\n",
      "40  16           None            None          None      2023-04-09\n",
      "41  18           None            None          None      2023-04-09\n",
      "42  19           None            None          None      2023-04-09\n",
      "43  22           None            None          None      2023-04-09\n",
      "44   2           None            None          None      2023-04-09\n",
      "45   8           None            None          None      2023-04-09\n",
      "46  13           None            None          None      2023-04-09\n",
      "47  16           None            None          None      2023-04-09\n",
      "48  18           None            None          None      2023-04-09\n",
      "49  19           None            None          None      2023-04-09\n",
      "50  22           None            None          None      2023-04-09\n"
     ]
    }
   ],
   "source": [
    "# Extract the job IDs from the filtered documents\n",
    "\n",
    "job_ids =[]\n",
    "for doc in filtered_docs2:\n",
    "    job_ids.append(doc['ID'])\n",
    "\n",
    "#print(job_ids)\n",
    "\n",
    "# Find the recruiter URLs in the JOBS_Recruiter collection that correspond to the filtered job IDs\n",
    "filtered_recruiters = collection2.find({'ID': {'$in': job_ids}})\n",
    "\n",
    "JOBS4=[]\n",
    "\n",
    "# Print the matching recruiter URLs\n",
    "print(\"Here are the Recruiter profiles!\")\n",
    "for recruiter in filtered_recruiters:\n",
    "    #if(recruiter['Recruiter URL']):\n",
    "    JOBS4.append((recruiter['ID'],recruiter['Recruiter Name'],recruiter['Recruiter Title'],recruiter['Recruiter URL'], recruiter['Date_Downloaded']))\n",
    "\n",
    "df4 = pd.DataFrame(JOBS4, columns=['ID', 'Recruiter Name','Recruiter Title','Recruiter URL','Date_Downloaded'])\n",
    "print(df4)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d056010c",
   "metadata": {},
   "source": [
    "### 4. By Job Post Date and CA Location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4de7fbc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5 jobs!\n",
      "Empty DataFrame\n",
      "Columns: [ID, TITLE, COMPANY, LOCATION, #Applicants, Date Posted, Job URL, Company URL, Date_Downloaded]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "# Find all documents in the JOBS_Basic collection that have \"hours ago\" in the \"DATE POSTED\" field \n",
    "# and \"Pune\" in the \"LOCATION\" field\n",
    "\n",
    "filtered_docs3 = collection1.find({'$and': [{'Date Posted': {'$regex': '.*hours ago.*'}},\n",
    "                                            {'LOCATION': {'$regex': '.*Pune.*'}}]})\n",
    "\n",
    "n3 = collection1.count_documents({'$and': [{'Date Posted': {'$regex': '.*hours ago.*'}},\n",
    "                                            {'LOCATION': {'$regex': '.*CA.*'}}]})\n",
    "\n",
    "print(\"Found %d jobs!\"%n3)\n",
    "\n",
    "JOBS5=[]\n",
    "# Print the matching documents\n",
    "for doc in filtered_docs3:\n",
    "    JOBS5.append([doc['ID'], doc['TITLE'], doc['COMPANY'], doc['LOCATION'], doc['#Applicants'],doc['Date Posted'],doc['Job URL'],doc['Company URL'],doc['Date_Downloaded'] ])\n",
    "\n",
    "df5 = pd.DataFrame(JOBS3, columns=['ID', 'TITLE', 'COMPANY', 'LOCATION', '#Applicants', 'Date Posted', 'Job URL', 'Company URL', 'Date_Downloaded'])\n",
    "print(df5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "vscode": {
   "interpreter": {
    "hash": "5b974e5b0282bcf0adbf7f147bc3cbccd5f91b0d65898b5cfbe5a8a858eda437"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
